{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1004.eqiad.wmnet:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2ae3146d68>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.getValue>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value info in wikidata entity table (https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Wikidata_entity)\n",
    "# is a string as opposed to struct (because it has a variable schema)\n",
    "# this UDF extracts the QID value (or null if doesn't exist)\n",
    "def getValue(obj):\n",
    "    try:\n",
    "        d =  eval(obj)\n",
    "        return d.get('id')\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "spark.udf.register('getValue', getValue, 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = '2020-11-02'\n",
    "wiki_db = 'enwiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WITH relevant_qids AS (\n",
      "    SELECT DISTINCT item_id\n",
      "      FROM wmf.wikidata_item_page_link\n",
      "     WHERE snapshot = '2020-11-02'\n",
      "           AND page_namespace = 0\n",
      "           AND wiki_db = 'enwiki'\n",
      "),\n",
      "exploded_statements AS (\n",
      "    SELECT id as item_id,\n",
      "           explode(claims) as claim\n",
      "      FROM wmf.wikidata_entity w\n",
      "     INNER JOIN relevant_qids q\n",
      "           ON (w.id = q.item_id)\n",
      "     WHERE w.snapshot = '2020-11-02'\n",
      "),\n",
      "relevant_statements AS (\n",
      "    SELECT item_id,\n",
      "           CONCAT_WS(\" \", Array(claim.mainSnak.property, getValue(claim.mainSnak.dataValue.value))) AS statement_str\n",
      "     FROM exploded_statements\n",
      ")\n",
      "SELECT item_id,\n",
      "       CONCAT_WS(\" \", COLLECT_LIST(statement_str)) AS statements\n",
      "  FROM relevant_statements\n",
      " GROUP BY item_id\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_for_hive = False\n",
    "do_execute = True\n",
    "\n",
    "query = \"\"\"\n",
    "WITH relevant_qids AS (\n",
    "    SELECT DISTINCT item_id\n",
    "      FROM wmf.wikidata_item_page_link\n",
    "     WHERE snapshot = '{0}'\n",
    "           AND page_namespace = 0\n",
    "           AND wiki_db = '{1}'\n",
    "),\n",
    "exploded_statements AS (\n",
    "    SELECT id as item_id,\n",
    "           explode(claims) as claim\n",
    "      FROM wmf.wikidata_entity w\n",
    "     INNER JOIN relevant_qids q\n",
    "           ON (w.id = q.item_id)\n",
    "     WHERE w.snapshot = '{0}'\n",
    "),\n",
    "relevant_statements AS (\n",
    "    SELECT item_id,\n",
    "           CONCAT_WS(\" \", Array(claim.mainSnak.property, getValue(claim.mainSnak.dataValue.value))) AS statement_str\n",
    "     FROM exploded_statements\n",
    ")\n",
    "SELECT item_id,\n",
    "       CONCAT_WS(\" \", COLLECT_LIST(statement_str)) AS statements\n",
    "  FROM relevant_statements\n",
    " GROUP BY item_id\n",
    "\"\"\".format(snapshot, wiki_db)\n",
    "\n",
    "if print_for_hive:\n",
    "    print(re.sub(' +', ' ', re.sub('\\n', ' ', query)).strip())\n",
    "else:\n",
    "    print(query)\n",
    "\n",
    "if do_execute:\n",
    "    result = spark.sql(query)\n",
    "    result.write.csv(path=\"/user/isaacj/wikidata-statements-enwiki\", compression=\"gzip\", header=True, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
